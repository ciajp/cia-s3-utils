require 'aws/s3'
require 'zip'
require 'active_support'
require 'active_support/core_ext/object/blank'
require "action_dispatch/http/mime_type"

module Cia
  module S3
    module Utils

      class S3Upload
        attr_reader :total_files, :s3_bucket, :threads, :uploaded_file
        attr_accessor :files

        PROGRESS_KEY=:progress
        TOTAL_KEY=:total

        # Initialize the upload class
        #
        # inspired by http://avi.io/blog/2013/12/03/upload-folder-to-s3-recursively
        #
        # zip_path - path to the zip that you want to upload
        # bucket - The bucket you want to upload to
        # stop_dir - Directory path you want to store
        # aws_key - Your key generated by AWS defaults to the environemt setting AWS_KEY_ID
        # aws_secret - The secret generated by AWS
        # aws_region - The region you want to use
        # aws_endpoint - The endpoint of s3 bucket
        #
        # Examples
        #   => uploader = S3ZipExtractUpload.new("some_route/zip_file", 'your_bucket_name', 'path/to/store')
        #
        def initialize(bucket, store_dir = '', progress = {}, aws_key = ENV['AWS_KEY_ID'], aws_secret = ENV['AWS_SECRET'], region = ENV['AWS_REGION'], aws_end_point = ENV['AWS_END_POINT'])

          @files          = []
          @uploaded_file  = []
          @total_files    = 0
          @progress       = progress

          @connection       = AWS::S3.new(access_key_id: aws_key, secret_access_key: aws_secret, region: region)#, s3_endpoint: aws_end_point
          @s3_bucket        = @connection.buckets[bucket]
          @dir              = store_dir
        end

        # public: Upload files from the folder to S3
        #
        # thread_count - How many @threads you want to use (defaults to 5)
        #
        # Examples
        #   => uploader.upload!(file/to/path, 20)
        #     true
        #   => uploader.upload!(file/to/path)
        #     true
        #
        # Returns true when finished the process
        def upload!(file_path, zip_extract = false, thread_count = 5)
          file_number = 0
          mutex       = Mutex.new
          @threads     = []
          is_zip = false

          if File.exist?(file_path) && zip_extract
            Zip::File.open(file_path) do | zipfile |
                zipfile.each do | entry |
                    next if entry.name =~ /(.*\.DS_Store|.*__MACOSX.*)/
                    @files << entry
                end
            end
            is_zip = true
          else
            @files << file_path
          end

          @total_files      = files.length
          @progress[TOTAL_KEY] = @total_files

          thread_count.times do |i|
            @threads[i] = Thread.new {
              until @files.empty?
                mutex.synchronize do
                  file_number += 1
                  Thread.current["file_number"] = file_number
                end
                file = @files.pop rescue nil
                next unless file

                path = "#{@dir}/#{file}"

                if is_zip && file.ftype == :file

                  # I had some more manipulation here figuring out the git sha
                  # For the sake of the example, we'll leave it simple
                  #

                  puts "[#{Thread.current["file_number"]}/#{@total_files}] uploading... #{path}"
                  @progress[PROGRESS_KEY] = Thread.current["file_number"]

                  ext = Pathname.new(file.name).extname.gsub(/\./, '').to_s
                  obj = s3_bucket.objects[path]
                  obj.write(file.get_input_stream.read , {acl: :public_read, content_type: ::Mime::Type.lookup_by_extension(ext).to_s})
                  @uploaded_file << file

                elsif !is_zip

                  puts "[#{Thread.current["file_number"]}/#{@total_files}] uploading..."
                  @progress[PROGRESS_KEY] = Thread.current["file_number"]

                  File.open(file) do |data|
                    next if File.directory?(data)
                    obj = s3_bucket.objects[path]
                    obj.write(data, { acl: :public_read })
                    @uploaded_file << file
                  end

                end
                if block_given?
                  yield Thread.current["file_number"], @total_files
                end

              end
            }
          end
          @threads.each { |t| t.join }
        end

        def delete(path, exclude_paths = nil, thread_count = 5)

          file_number = 0
          mutex       = Mutex.new
          @threads    = []
          deleted = []
          objects = @s3_bucket.objects.with_prefix(path.to_s).to_a
          total_files = objects.length

          thread_count.times do |i|
            @threads[i] = Thread.new {
              until objects.empty?
        
                mutex.synchronize do
                  file_number += 1
                  Thread.current["file_number"] = file_number
                end

                obj = objects.pop rescue nil
                next unless obj

                puts "[#{Thread.current["file_number"]}/#{total_files}] deleting..."

                unless (!exclude_paths.nil? && 
                  ((exclude_paths.kind_of?(Array) && exclude_paths.select {|path| obj.key.include?(path) }.present?) ||
                  (exclude_paths.kind_of?(String) && obj.key.include?(exclude_paths))) )
                  deleted << obj
                  obj.delete
                end
                
              end
            }
          end

          @threads.each { |t| t.join }

          deleted.length
        end

        def restrict_acl(path)
          @s3_bucket.objects[path].acl = :private if @s3_bucket.objects[path].exists?
        end

        #
        # 指定パスの権限付きURL取得
        #
        def url_for(path)
          @s3_bucket.objects[path].url_for(:read) if @s3_bucket.objects[path].exists?
        end

        def publish(path)
          @s3_bucket.objects[path].acl = :public_read if @s3_bucket.objects[path].exists?    
        end
      end

    end
  end
end
